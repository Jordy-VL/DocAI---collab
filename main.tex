\documentclass{article}[letterpaper]
\usepackage[utf8]{inputenc}
\usepackage{natbib}

\input{preamble/preamble}

\begin{document}

\title{DocAI - collab: working document}
\author{TBD}
\date{June 2022}


\maketitle

\section{Introduction}

\subsubsection{Motivation}

Find a topic in document AI (DocAI) which serves our joint interests and warrants a collaboration.

\subsubsection{Context}

Major challenges in DocAI include:
\begin{enumerate}
    \item business value lies in confidently and correctly predicting page classification - KIE - LIE; no standard evaluation methodology between providers
    \item absence of high-quality opensource datasets that extend to multi-page documents
    \begin{itemize}
        \item requires sample-efficient to few-shot approaches for KIE under data sparsity 
    \end{itemize}
    \item current language modeling overly focused on unstructured wikipedia-like text (Commoncrawl only has <0.5\% pdfs)
\end{enumerate}


\section{Related Work}

\citep{cui2021document} makes a survey on the field of DocAI, contains references to some lesser known datasets.

\citep{rossum2022practicalbenchmarks} ask a similar question on practical DocAI benchmarks under opensource data scarcity, proposes synthetic generation with rich layout diversity (in future work).

The DUE benchmark \citep{borchmann2021due} consists of available and reformulated datasets to measure the end-to-end capabilities of DocAI systems.

New OCR dataset of industry multi-page documents \citep{biten2022ocr}. 

\bibliography{main.bib}

\clearpage

\appendix

\section{Meeting notes}

\subsection{10-06-2022}

General discussion on three ideas for papers. 

\begin{enumerate}
    \item Joint document image classification \&  KIE
    \begin{itemize}
        \item add explicit objective function for joint top-1 calibration 
    \end{itemize}
    \item Sample-efficient KIE with pattern-minded Transformers for reducing annotation costs
    \begin{itemize}
        \item Intuition: "If you know how to predict an invoicenumber, predicting a ...number" should not be hard
        \item current subword tokenization treats KIE entities poorly, remediate by including "pattern embeddings"
        \item pretrain-then-finetune paradigm yet try to treat by finetuning-only early fusion, replacing first encoder (common in CV) and decision layer
        \item might require specific optimization scheme focused on novel parameters (cf. layer-wise adaptive learning rate, gradual unfreezing)
        \item Problem: opensource dataset which has very patternlike entities to be extracted
    \end{itemize}
    \item A comparison tool for out-of-the-box performance on domain-specific data [API caller meets DUE].
    \begin{itemize}
        \item As a potential buyer of a DocAI solution, I would like to compare the out-of-the-box offering on my test dataset ASAP
        \item Create a frictionless experience, build translators from their scheme to open APIs like Azure, Google Cloud, Amazon Textract, ...
    \end{itemize}
\end{enumerate}

\subsubsection{Calibration primer}

%%https://arxiv.org/pdf/2102.10809.pdf
\begin{definition}[Perfect calibration\small] \label{def:calibration}
    \citep{dawid1982well,degroot1983comparison,zadrozny2002transforming}
    Calibration is a property of an empirical predictor $f$, which states that on finite-sample data it converges to a solution where the scoring function reflects the probability $\rho$ of being correct. Perfect calibration, $\mathrm{CE}(f) = 0$, is satisfied if:
    \begin{equation}
        \mathbb{P}(Y=\hat{Y} \mid f(X)=\rho)=\rho, \quad \forall \rho \in[0,1]
    \end{equation}
\end{definition}

For notational convenience, we indicate a ground truth class $y$ as an index in a pre-defined subset of classes, which when used as an index of $f(x)$, corresponds to a probability vector for class $y$.  

\begin{definition}[\textit{Top-1 calibration}]
    \label{def:1-calibration}
\normalfont
A model $f$ is said to be top-1-calibrated if, 
\begin{equation}
    \mathbb{P}(Y=\argmax_{k}^K f(X)_k \mid \max_{k}^K f(X)_k)= \max_{k}^K f(X)_k,
\end{equation}
almost surely.  
\end{definition}

\begin{definition}[\textit{Multi-task top-1 calibration}]
    \label{def:mt-calibration}
A set of $T$ models $F = \{f_{1}, ..., f_{T}\}$ is said to be \textit{jointly} top-1 calibrated if, 
\begin{equation}
    \mathbb{P}(Y=\argmax_{k}^K f(X)_k \mid \max_{k}^K f(X)_k)= \max_{k}^K f(X)_k \quad \forall f \in F,
\end{equation}
almost surely.  
\end{definition}

From \Cref{def:mt-calibration} we can define an empirical estimator, here the question becomes if discretization is shared over tasks or not.
Intuitively, each $f$ can have predicted probabilities $\hat{p}$ concentrated in different regions of the simplex, so it would make sense to treat each separately.
Would there be any gains in different discretization (histogram binning, spline, kernel)? The loss function can be similar to $\operatorname{MMCE}$ \citep{kumar2018trainable}. 

First idea: play around with imbalance weighting within MMCE, since each $f$ can have different number of (expected) predictions and intrinsic correctness $N_f$.
%1: incorrect
%2: correct
%3: different
% logic: N_c/N to correct 

\begin{align}
    \widehat{\operatorname{MMCE}}^2_w(f,\kappa)
    & = 
    \sum_{\substack{c_i=c_j=0}}  \frac{\tilde{p}_i \tilde{p}_j \ \kappa(\tilde{p}_i,\tilde{p}_j)}{N_c^2} \\
    &+ \sum_{\substack{c_i=c_j=1}}  \frac{ (1-\tilde{p}_i) (1-\tilde{p}_j) \ \kappa(\tilde{p}_i,\tilde{p}_j)}{(N - N_c)^2} \\
    &- 2 \sum_{\substack{c_i=1 \\c_j=0}}  \frac{(1-\tilde{p}_i) \tilde{p}_j \ \kappa(\tilde{p}_i,\tilde{p}_j)}{(N_c N)} 
\end{align}



\subsection{24-06-2022}

\textbf{Dataset construction for multi-page document VQA}

I. EU legal acts (\url{https://eur-lex.europa.eu/homepage.html})

II. Public EU data (\url{https://data.europa.eu/en} , \url{https://data.europa.eu/data/datasets?catalog=data-gov-uk&showcatalogdetails=true&locale=en&format=PDF&page=1})

III. Patents (\url{https://patents.google.com/}) , check also (\url{https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset})

IV. Consumer product labels (Unilever or opensource: \url{https://world.openfoodfacts.org/})

\subsubsection{Actions}

First check on public document data that we could annotate

Get contacts of potential dataset construction collaborators (BCN infographics ({abiten,rperez,lgomez,ernest,dimos
}@cvc.uab.cat), La Rochelle University (mickael.coustaty@univ-lr.fr))



\section{13-07-2022}

\subsection{Actions}

Defining the perfect, Bayes optimal thing.
 

Competition: need both training \& test set \& generalization set - [diversified test-set \& completely different [unseen templates]]

Within domain of real-life [business] documents:
	\begin{itemize}
		\item different vendors
		\item different layouts
		\item different domains and document types [medical, finance, legal, ...]
		\item sub-datasets addresses specific layouts/problems [checkboxes in medical; multi-header tables in shipping/literature]
	\end{itemize}

This will require metadating the documents and building generalization dataset. On the level of questions and templates, not $iid$. Even leave out a whole domain.

X - (in)finite data - synthetic/natural - languages - domains - modalities (image-text-layout) - native/scanned - OCR ground truth 

Y - annotations (complexity dependent on format) - formats [DLA, OCR, KIE] - (harder or easy to gather) - DocQA is easier, only document-level annotations and text-to-text evaluation - postprocessing (different normalization or multiple possible ground truths, e.g., dates in different formats) - annotation scheme complexity (how hierarchical/specific you want to make it; v1 and v2) - annotation layers (token OCR bbox annotation for extractive/generative answers)

\subsection{Obtaining data - sources}

Methodology for data creation. 

\href{https://docs.google.com/spreadsheets/d/152hSv5CBsAeqJCCv81jztb_5rdlwP7mQV5cEh99Lqq4/edit?usp=sharing}{Link to dataset metadata}

Generic DocVQA questions:
\begin{enumerate}
	\item document date
	\item ...
\end{enumerate}

Domain questions;
\begin{enumerate}
	\item amount-based entity questions in finance documents
	\item table statistic in research document
\end{enumerate}

\subsubsection{Exercise on data set choice}

Initial trial-and-error: is the dataset source challenging and quite good?
Go through TILT model as a baseline. T5 + layout

1. how I will easily create those? 

Limited layout diversity in consumer food product images.



\end{document}