\documentclass{article}[letterpaper]
\usepackage[utf8]{inputenc}
\usepackage{natbib}

\input{preamble/preamble}


\begin{document}

\title{DocAI - collab: working document}
%\author{TBD}
\date{June 2022}


\maketitle

\section{Introduction}

\subsubsection{Motivation}

Find a topic in document AI (DocAI) which serves our joint interests and warrants a collaboration.

\subsubsection{Context}

Major challenges in DocAI include:
\begin{enumerate}
    \item business value lies in confidently and correctly predicting page classification - KIE - LIE; no standard evaluation methodology between providers
    \item absence of high-quality opensource datasets that extend to multi-page documents
    \begin{itemize}
        \item requires sample-efficient to few-shot approaches for KIE under data scarcity 
    \end{itemize}
    \item current language modeling overly focused on unstructured wikipedia-like text (Commoncrawl only has <0.5\% pdfs)
    \begin{itemize}
        \item structured text contains entities which are not always retrievable by context; more by layout or relative positioning
        \item could enhance input text with entity/pattern annotations \citep{paolini2021structured, qian-etal-2021-structural,ye2021pack}
    \end{itemize}
\end{enumerate}


\section{Related Work}

\citep{cui2021document} makes a survey on the field of DocAI, contains references to some lesser known datasets.

\citep{rossum2022practicalbenchmarks} ask a similar question on practical DocAI benchmarks under opensource data scarcity, proposes synthetic generation with rich layout diversity (in future work).

The DUE benchmark \citep{borchmann2021due} consists of available and reformulated datasets to measure the end-to-end capabilities of DocAI systems.

New OCR dataset of industry multi-page documents \citep{biten2022ocr}. 

\bibliography{main.bib}

\clearpage

\appendix

\section{Meeting notes}

\subsection{10-06-2022}

General discussion on three ideas for papers. 

\begin{enumerate}
    \item Joint document image classification \&  KIE
    \begin{itemize}
        \item add explicit objective function for joint top-1 calibration 
    \end{itemize}
    \item Sample-efficient KIE with pattern-minded Transformers for reducing annotation costs
    \begin{itemize}
        \item Intuition: "If you know how to predict an invoicenumber, predicting a ...number" should not be hard
        \item current subword tokenization treats KIE entities poorly, remediate by including "pattern embeddings"
        \item pretrain-then-finetune paradigm yet try to treat by finetuning-only early fusion, replacing first encoder (common in CV) and decision layer
        \item might require specific optimization scheme focused on novel parameters (cf. layer-wise adaptive learning rate, gradual unfreezing)
        \item Problem: opensource dataset which has very patternlike entities to be extracted
    \end{itemize}
    \item A comparison tool for out-of-the-box performance on domain-specific data [API caller meets DUE].
    \begin{itemize}
        \item As a potential buyer of a DocAI solution, I would like to compare the out-of-the-box offering on my test dataset ASAP
        \item Create a frictionless experience, build translators from their scheme to open APIs like Azure, Google Cloud, Amazon Textract, ...
    \end{itemize}
\end{enumerate}



\subsection{24-06-2022}

\textbf{Dataset construction for multi-page document VQA}

I. EU legal acts (\url{https://eur-lex.europa.eu/homepage.html})

II. Public EU data (\url{https://data.europa.eu/en} , \url{https://data.europa.eu/data/datasets?catalog=data-gov-uk&showcatalogdetails=true&locale=en&format=PDF&page=1})

III. Patents (\url{https://patents.google.com/}) , check also (\url{https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset})

IV. Consumer product labels (Unilever or opensource: \url{https://world.openfoodfacts.org/})

V. Pharmacy prescriptions

\subsubsection{Actions}

First check on public document data that we could annotate

Get contacts of potential dataset construction collaborators (BCN infographics ({abiten,rperez,lgomez,ernest,dimos
}@cvc.uab.cat), La Rochelle University (mickael.coustaty@univ-lr.fr))



\section{13-07-2022}

\subsection{Actions}

Defining the perfect, Bayes optimal thing.
 

Competition: need both training \& test set \& generalization set - [diversified test-set \& completely different [unseen templates]]

Within domain of real-life [business] documents:
	\begin{itemize}
		\item different vendors
		\item different layouts
		\item different domains and document types [medical, finance, legal, ...]
		\item sub-datasets addresses specific layouts/problems [checkboxes in medical; multi-header tables in shipping/literature]
	\end{itemize}

This will require metadating the documents and building generalization dataset. On the level of questions and templates, not $iid$. Even leave out a whole domain.

X - (in)finite data - synthetic/natural - languages - domains - modalities (image-text-layout) - native/scanned - OCR ground truth 

Y - annotations (complexity dependent on format) - formats [DLA, OCR, KIE] - (harder or easy to gather) - DocQA is easier, only document-level annotations and text-to-text evaluation - postprocessing (different normalization or multiple possible ground truths, e.g., dates in different formats) - annotation scheme complexity (how hierarchical/specific you want to make it; v1 and v2) - annotation layers (token OCR bbox annotation for extractive/generative answers)

\subsection{Obtaining data - sources}

Methodology for data creation. 

\href{https://docs.google.com/spreadsheets/d/152hSv5CBsAeqJCCv81jztb_5rdlwP7mQV5cEh99Lqq4/edit?usp=sharing}{Link to dataset metadata}

Generic DocVQA questions:
\begin{enumerate}
	\item document date
	\item ...
\end{enumerate}

Domain questions;
\begin{enumerate}
	\item amount-based entity questions in finance documents
	\item table statistic in research document
\end{enumerate}

\subsubsection{Exercise on data set choice}

Initial trial-and-error: is the dataset source challenging and quite good?
Go through TILT model as a baseline. T5 + layout

1. how will we easily create those? 

Limited layout diversity in consumer food product images.


\section{19-07-2022}

\begin{enumerate}
	\item Data sources
	\item Challenges in creating challenging benchmarks: annotation building
	\begin{enumerate}
		\item modalities (text, layout, image)
		\item inference structure / answer complexity (single answer, table, cell within table; sum over cells)
		\item question complexity (abstractive/extractive; mathematical operations)
		\item context helpful to understand layout (give some coordination: first row of 2nd table on the page 1) $\to$ keywords should help
		\begin{itemize}
			\item true layout understanding and navigation
			\item absolute \& relative questions
			\item require ground truth layout, might come from generative process 
		\end{itemize}
	\end{enumerate}
\end{enumerate}

Important references:
\begin{itemize}
	\item \href{https://openreview.net/pdf/f3dad930cb55abce99a229e35cc131a2db791b66.pdf}{MULTIMODALQA: COMPLEX QUESTION ANSWERING
		OVER TEXT, TABLES AND IMAGES}
	\item \href{https://arxiv.org/pdf/2105.07624.pdf}{TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and
		Textual Content in Finance}
\end{itemize}

Evaluation
\begin{itemize}
	\item Augment test set in such a way that we scan each born digital document 
	\item Build in procedure to evaluate for attribute resistance/robustness (scan/not; template/not)
	\item generative models *could be* more robust to this, fixing OCR errors as it predicts; depends on pretraining task -> fix errors
\end{itemize}

\begin{enumerate}
	\item Sources
	\item How to annotate?
	\item Establish plan
	\item Exercise of creating some example annotations, understanding of complexity
	
\end{enumerate}


\end{document}